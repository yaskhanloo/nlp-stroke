{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Named Entity Recognition (NER) \u2013 English Stroke Report Prototype\n",
        "\n",
        "This notebook shows a minimal working example for NER using a synthetic English sentence relevant to stroke MRI reports. It uses `transformers`, `datasets`, and `evaluate` with the `bert-base-cased` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "!pip install transformers datasets evaluate torch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample synthetic data (token-level labels in BIO format)\n",
        "examples = [\n",
        "    {\n",
        "        \"tokens\": [\"The\", \"patient\", \"received\", \"10\", \"mg\", \"rtPA\", \"under\", \"general\", \"anesthesia\", \".\"],\n",
        "        \"ner_tags\": [\"O\", \"O\", \"O\", \"B-DOSE\", \"I-DOSE\", \"B-DRUG\", \"O\", \"B-ANESTH\", \"I-ANESTH\", \"O\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "ner_dataset = Dataset.from_list(examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define label list and map\n",
        "label_list = [\"O\", \"B-DOSE\", \"I-DOSE\", \"B-DRUG\", \"B-ANESTH\", \"I-ANESTH\"]\n",
        "label2id = {l: i for i, l in enumerate(label_list)}\n",
        "id2label = {i: l for l, i in label2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize and align labels\n",
        "def tokenize_and_align_labels(example):\n",
        "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "    label_ids = []\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            label_ids.append(label2id[example[\"ner_tags\"][word_idx]])\n",
        "        else:\n",
        "            label_ids.append(label2id[example[\"ner_tags\"][word_idx]])\n",
        "        previous_word_idx = word_idx\n",
        "    tokenized_inputs[\"labels\"] = label_ids\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_dataset = ner_dataset.map(tokenize_and_align_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=len(label_list), id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate (dummy run on one example)\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "outputs = model(**{k: tokenized_dataset[0][k][None] for k in [\"input_ids\", \"attention_mask\"]})\n",
        "logits = outputs.logits\n",
        "predictions = logits.argmax(dim=-1)\n",
        "labels = tokenized_dataset[0][\"labels\"]\n",
        "\n",
        "# Map back to labels\n",
        "predicted_labels = [id2label[int(p)] for p in predictions[0] if p != -100]\n",
        "true_labels = [id2label[int(l)] for l in labels if l != -100]\n",
        "\n",
        "print(\"PRED:\", predicted_labels)\n",
        "print(\"TRUE:\", true_labels)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (nlp-stroke)",
      "language": "python",
      "name": "nlp-stroke"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}