{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition for German MRI Reports (Synthetic Example)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset, load_metric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Generate Synthetic Reports\n",
    "# ----------------------------\n",
    "synthetic_reports = [\n",
    "    \"Patient erhielt intraarteriell 10 mg rtPA unter Vollnarkose mittels Stent Retriever.\",\n",
    "    \"Durchgef체hrt wurde eine Aspiration ohne Komplikationen, rtPA nicht gegeben.\",\n",
    "    \"Urokinase intraarteriell in Dosis von 5 mg verwendet, gefolgt von Ballonangioplastie.\",\n",
    "    \"Kein Stent, keine An채sthesie, keine EVT-Komplikationen.\",\n",
    "    \"Thrombektomie mit Distal Retriever durchgef체hrt.\"\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    [\"O\", \"O\", \"B-Intraarterial_rtPA\", \"B-rtPA_dose\", \"I-rtPA_dose\", \"I-rtPA_dose\", \"O\", \"B-Anesthesia\", \"O\", \"B-Stent_Retriever\", \"O\"],\n",
    "    [\"B-Mechanical_Treatment\", \"O\", \"O\", \"O\", \"O\", \"B-Intraarterial_rtPA\", \"O\", \"O\"],\n",
    "    [\"B-Intraarterial_Urokinase\", \"O\", \"O\", \"B-Urokinase_dose\", \"I-Urokinase_dose\", \"O\", \"O\", \"B-Balloon_Angioplasty\", \"O\"],\n",
    "    [\"O\", \"B-Anesthesia\", \"O\", \"O\", \"B-EVT_Complications\", \"O\"],\n",
    "    [\"B-Mechanical_Treatment\", \"O\", \"B-Distal_Retriever\", \"O\"]\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Tokenizer & Label Mapping\n",
    "# ----------------------------\n",
    "model_checkpoint = \"bert-base-german-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "label_list = list({tag for sent in labels for tag in sent if tag != 'O'})\n",
    "label_list = ['O'] + sorted(label_list)\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "# Tokenize and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels_aligned = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label2id[label[word_idx]] if label[word_idx].startswith(\"I-\") else label2id[label[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        labels_aligned.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels_aligned\n",
    "    return tokenized_inputs\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Prepare Dataset\n",
    "# ----------------------------\n",
    "df = pd.DataFrame({\"tokens\": [r.split() for r in synthetic_reports], \"ner_tags\": labels})\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Load Model & Train\n",
    "# ----------------------------\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"ner-mri-german\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Predict on New Text\n",
    "# ----------------------------\n",
    "text = \"Der Patient erhielt intraarteriell 10 mg rtPA und eine Aspiration wurde durchgef체hrt.\"\n",
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs).logits\n",
    "predictions = torch.argmax(outputs, dim=2)\n",
    "\n",
    "print(\"\\nPredicted Entities:\")\n",
    "for token, pred_id in zip(tokens, predictions[0][1:-1]):  # skip [CLS] and [SEP]\n",
    "    label = id2label[pred_id.item()]\n",
    "    if label != \"O\":\n",
    "        print(f\"{token} -> {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
